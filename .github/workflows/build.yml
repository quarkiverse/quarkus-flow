name: Build

on:
  push:
    branches: [ "main" ]
    paths-ignore: [ ".gitignore", "CODEOWNERS", "LICENSE", "*.md", "*.adoc", "*.txt", ".all-contributorsrc", ".github/project.yml" ]
  pull_request:
    paths-ignore: [ ".gitignore", "CODEOWNERS", "LICENSE", "*.md", "*.adoc", "*.txt", ".all-contributorsrc" ]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash

jobs:
  build-linux:
    name: Build & test (Ubuntu with Ollama)
    runs-on: ubuntu-latest
    env:
      OLLAMA_TIMEOUT: ${{ vars.OLLAMA_TIMEOUT }}
    steps:
      - uses: actions/checkout@v5

      - name: Set up JDK 17
        uses: actions/setup-java@v5
        with:
          distribution: temurin
          java-version: 17
          cache: maven

      - name: Restore Ollama models cache
        uses: actions/cache@v4
        with:
          path: ~/.ollama
          key: ${{ runner.os }}-ollama-${{ hashFiles('.github/ollama-models.txt') }}

      - name: Setup Ollama CLI
        uses: ai-action/setup-ollama@v1

      - name: Pre-pull models (tiny default)
        run: |
          # List the models in testsâ€”one per line
          xargs -n1 -I{} bash -c 'echo "Pulling {}"; ollama pull "{}"' < .github/ollama-models.txt

      - name: Set global LangChain4j timeout (with default)
        run: |
          # Fallback to 60s if OLLAMA_TIMEOUT is empty
          : "${OLLAMA_TIMEOUT:=60s}"
          echo "Using QUARKUS_LANGCHAIN4J_OLLAMA_TIMEOUT=$OLLAMA_TIMEOUT"
          echo "QUARKUS_LANGCHAIN4J_OLLAMA_TIMEOUT=$OLLAMA_TIMEOUT" >> "$GITHUB_ENV"

      - name: Wait for Ollama & warm up model(s)
        shell: bash
        run: |
          set -Eeuo pipefail

          for i in {1..60}; do
            if curl -sf http://localhost:11434/api/tags >/dev/null; then break; fi
            sleep 1
          done
          
          # Warm each model listed in .github/ollama-models.txt
          while IFS= read -r line || [[ -n "${line:-}" ]]; do
            model="${line%$'\r'}"
            [[ -z "${model}" || "${model}" == \#* ]] && continue    
            echo "Warming up ${model}"
            curl -s -X POST http://localhost:11434/api/generate \
              -H 'Content-Type: application/json' \
              -d "{\"model\":\"${model}\",\"prompt\":\"say ok\",\"stream\":false,\"keep_alive\":\"30m\",\"options\":{\"num_predict\":8}}" \
              >/dev/null || true
          done < .github/ollama-models.txt

      - name: Build with Maven
        run: mvn -B clean install -Dno-format

      - name: Build with Maven (Native)
        run: mvn -B install -Dnative -Dquarkus.native.container-build -Dquarkus.native.builder-image=quay.io/quarkus/ubi-quarkus-mandrel-builder-image:jdk-25 -DskipITs=true
  build-windows:
    name: Build only (Windows)
    runs-on: windows-latest
    steps:
      - name: Prepare git (CRLF)
        run: git config --global core.autocrlf false

      - uses: actions/checkout@v5

      - name: Set up JDK 17
        uses: actions/setup-java@v5
        with:
          distribution: temurin
          java-version: 17
          cache: maven

      - name: Build with Maven (skip agentic tests)
        run: mvn -B -Dno-format -DskipITs=true -Dquarkus.langchain4j.ollama.devservices.enabled=false clean install