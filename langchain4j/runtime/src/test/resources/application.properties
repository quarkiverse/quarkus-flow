quarkus.langchain4j.log-responses=true
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.timeout=60000

# Ollama Configuration
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.temperature=0.2
quarkus.langchain4j.ollama.chat-model.top-p=0.8
quarkus.langchain4j.ollama.chat-model.top-k=20
quarkus.langchain4j.ollama.chat-model.num-predict=512
quarkus.langchain4j.ollama.chat-model.seed=42

quarkus.log.category."io.quarkiverse.flow".level=DEBUG