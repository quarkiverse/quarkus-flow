quarkus.langchain4j.log-responses=false
quarkus.langchain4j.log-requests=true


quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.temperature=0.2
quarkus.langchain4j.ollama.chat-model.top-p=0.8
quarkus.langchain4j.ollama.chat-model.top-k=20
quarkus.langchain4j.ollama.chat-model.format=json
quarkus.langchain4j.ollama.chat-model.num-predict=512
quarkus.langchain4j.ollama.chat-model.seed=42
